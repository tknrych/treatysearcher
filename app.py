import streamlit as st
import urllib.parse

<<<<<<< HEAD
# ---------- åˆæœŸåŒ– ----------
load_dotenv()

# ---------- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ----------
@st.cache_resource
def init_db():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’åˆæœŸåŒ–ã—ã€ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã™"""
    db_path = os.getenv("DATABASE_PATH")
    return sqlite3.connect(db_path, check_same_thread=False)

@st.cache_data  # ã“ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’è¿½åŠ 
def load_all_terms(_conn):
    """DBã‹ã‚‰å…¨ç”¨èªã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰"""
    cursor = _conn.cursor()
    cursor.execute("SELECT english_term, japanese_term FROM terms")
    return cursor.fetchall()

def find_glossary_terms(text: str, conn) -> dict:
    """
    ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰ç”¨èªé›†ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹è‹±å˜èªã‚’æ­£è¦è¡¨ç¾ã§åŠ¹ç‡çš„ã«æ¤œç´¢ã—ã€
    { è‹±å˜èª: [æ—¥æœ¬èªè¨³1, æ—¥æœ¬èªè¨³2, ...] } ã®å½¢å¼ã®è¾æ›¸ã§è¿”ã™
    """
    all_terms = load_all_terms(conn) # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’å‘¼ã³å‡ºã™
    if not all_terms:
        return {}

    # --- å¤‰æ›´ç‚¹ 1: 1ã¤ã®è‹±èªã«å¯¾ã—ã€è¤‡æ•°ã®æ—¥æœ¬èªè¨³ã‚’ãƒªã‚¹ãƒˆã§æ ¼ç´ã™ã‚‹ ---
    term_map = {}
    for en_term, ja_term in all_terms:
        if en_term not in term_map:
            term_map[en_term] = []
        term_map[en_term].append(ja_term)
    # ----------------------------------------------------------------

    # é•·ã„å˜èªã‹ã‚‰ãƒãƒƒãƒã•ã›ã‚‹ãŸã‚ã«ã€ã‚­ãƒ¼ï¼ˆè‹±å˜èªï¼‰ã®é•·ã•ã§é™é †ã‚½ãƒ¼ãƒˆ
    sorted_terms = sorted(term_map.keys(), key=len, reverse=True)

    # å˜èªå¢ƒç•Œ(\b)ã‚’ä½¿ã„ã€æ„å›³ã—ãªã„éƒ¨åˆ†ä¸€è‡´ã‚’é˜²ãæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
    pattern = r'\b(' + '|'.join(re.escape(term) for term in sorted_terms) + r')\b'

    # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«å‡ºç¾ã™ã‚‹ã™ã¹ã¦ã®ç”¨èªã‚’æ¤œç´¢
    matches = re.finditer(pattern, text, re.IGNORECASE)

    # è¦‹ã¤ã‹ã£ãŸå˜èªï¼ˆé‡è¤‡ãªã—ï¼‰ã¨ãã®æ—¥æœ¬èªè¨³ãƒªã‚¹ãƒˆã‚’æ ¼ç´ã™ã‚‹
    found_terms = {}
    # --- å¤‰æ›´ç‚¹ 2: lower_term_map ã‚‚ãƒªã‚¹ãƒˆã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ ---
    lower_term_map = {en.lower(): (en, ja_list) for en, ja_list in term_map.items()}
    # --------------------------------------------------------

    for match in matches:
        matched_text_lower = match.group(1).lower()
        if matched_text_lower in lower_term_map:
            # --- å¤‰æ›´ç‚¹ 3: ja_term ã§ã¯ãªã ja_list ã‚’å—ã‘å–ã‚‹ ---
            original_en, ja_list = lower_term_map[matched_text_lower]
            if original_en not in found_terms:
                found_terms[original_en] = ja_list
            # ---------------------------------------------------

    return found_terms

@st.cache_resource
def get_clients():
    search = SearchClient(
        endpoint=os.getenv("AZURE_SEARCH_ENDPOINT"),
        index_name=os.getenv("AZURE_SEARCH_INDEX"),
        credential=AzureKeyCredential(os.getenv("AZURE_SEARCH_API_KEY")),
    )
    aoai = AzureOpenAI(
        api_key=os.getenv("AZURE_AIS_OPENAI_API_KEY"),
        azure_endpoint=os.getenv("AZURE_AIS_OPENAI_ENDPOINT"),
        api_version=os.getenv("AZURE_AIS_OPENAI_API_VERSION")
    )
    gpt_model = os.getenv("AZURE_AIS_OPENAI_GPT_DEPLOYMENT")
    embed_model = os.getenv("AZURE_AIS_OPENAI_EMBED_DEPLOYMENT")
    return search, aoai, gpt_model, embed_model

search, aoai, GPT_MODEL, EMBED_MODEL = get_clients()

# --- æ—¥æœ¬èªã®å¯¾å¿œè¡¨ã‚’å®šç¾© ---
# å“è© (Universal POS tags) ã®æ—¥æœ¬èªå¯¾å¿œè¡¨
pos_tag_japanese = {
    'PROPN': 'å›ºæœ‰åè©', 'NOUN': 'åè©', 'VERB': 'å‹•è©', 'ADJ': 'å½¢å®¹è©',
    'ADV': 'å‰¯è©', 'ADP': 'å‰ç½®è©', 'AUX': 'åŠ©å‹•è©', 'CCONJ': 'ç­‰ä½æ¥ç¶šè©',
    'SCONJ': 'å¾“ä½æ¥ç¶šè©', 'DET': 'é™å®šè©', 'INTJ': 'é–“æŠ•è©', 'NUM': 'æ•°è©',
    'PART': 'åŠ©è©', 'PRON': 'ä»£åè©', 'PUNCT': 'å¥èª­ç‚¹', 'SYM': 'è¨˜å·',
    'X': 'ãã®ä»–', 'SPACE': 'ã‚¹ãƒšãƒ¼ã‚¹'
}
# ä¿‚ã‚Šå—ã‘é–¢ä¿‚ (Universal Dependency Relations) ã®æ—¥æœ¬èªå¯¾å¿œè¡¨
deprel_japanese = {
    'nsubj': 'åè©ä¸»èª', 'obj': 'ç›®çš„èª', 'iobj': 'é–“æ¥ç›®çš„èª', 'csubj': 'ç¯€ä¸»èª',
    'ccomp': 'ç¯€è£œèª', 'xcomp': 'åˆ¶å¾¡è£œèª', 'obl': 'æ–œæ ¼è£œèª', 'vocative': 'å‘¼æ ¼',
    'expl': 'è™šè¾', 'dislocated': 'è»¢ä½', 'advcl': 'å‰¯è©ç¯€ä¿®é£¾', 'advmod': 'å‰¯è©ä¿®é£¾',
    'discourse': 'è«‡è©±è¦ç´ ', 'aux': 'åŠ©å‹•è©', 'cop': 'ã‚³ãƒ”ãƒ¥ãƒ©', 'mark': 'æ¨™è­˜',
    'nmod': 'åè©ä¿®é£¾', 'appos': 'åŒæ ¼', 'nummod': 'æ•°è©ä¿®é£¾', 'acl': 'ç¯€ä¿®é£¾',
    'amod': 'å½¢å®¹è©ä¿®é£¾', 'det': 'é™å®šè©', 'clf': 'é¡åˆ¥è©', 'case': 'æ ¼è¡¨ç¤º',
    'conj': 'æ¥ç¶šè©', 'cc': 'ç­‰ä½æ¥ç¶šè©', 'fixed': 'å›ºå®šè¡¨ç¾', 'flat': 'å¹³å¦æ§‹é€ ',
    'compound': 'è¤‡åˆèª', 'list': 'ãƒªã‚¹ãƒˆ', 'parataxis': 'ä¸¦åˆ—', 'orphan': 'å­¤å…',
    'goeswith': 'é€£æ¥', 'reparandum': 'è¨‚æ­£', 'punct': 'å¥èª­ç‚¹', 'root': 'æ–‡ã®æ ¹',
    'dep': 'ä¸æ˜ãªä¾å­˜é–¢ä¿‚', 'prep': 'å‰ç½®è©ä¿®é£¾', 'agent': 'å‹•ä½œä¸»', 'attr': 'å±æ€§',
    'dobj': 'ç›´æ¥ç›®çš„èª', 'pobj': 'å‰ç½®è©ã®ç›®çš„èª', 'pcomp': 'å‰ç½®è©ã®è£œèª',
    'relcl': 'é–¢ä¿‚ç¯€ä¿®é£¾'
}

# ---------- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ----------
def is_japanese(text: str) -> bool:
    return re.search(r"[\u3040-\u30ff\u3400-\u9fff]", text) is not None

@Language.component("pysbd_sentencizer")
def pysbd_sentence_boundaries(doc):
    lang_code = "ja" if is_japanese(doc.text) else "en"
    seg = pysbd.Segmenter(language=lang_code, clean=False, char_span=True)
    sents_char_spans = seg.segment(doc.text)
    start_char_indices = {s.start for s in sents_char_spans}
    for token in doc:
        token.is_sent_start = True if token.i == 0 or token.idx in start_char_indices else False
    return doc

# ---------- NLPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ ----------
@st.cache_resource
def load_nlp_model(model_name="en_core_web_lg"):
    try:
        nlp = spacy.load(model_name)
        nlp.add_pipe("pysbd_sentencizer", before="parser")
        return nlp
    except OSError:
        st.warning(f"spaCyãƒ¢ãƒ‡ãƒ« '{model_name}' ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None

@st.cache_resource
def load_stanza_model():
    """Stanzaã®è‹±èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ä½¿ç”¨ã™ã‚‹ã€‚"""
    try:
        # GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        use_gpu = torch.cuda.is_available()      
        stanza.download('en')
        return stanza.Pipeline('en', use_gpu=use_gpu)
    except Exception as e:
        st.error(f"Stanzaãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

nlp = load_nlp_model()
stanza_nlp = load_stanza_model()

def _clear_title_tab_results():
    """ã‚¿ãƒ–ã€Œæ¡ç´„åæ¤œç´¢ã€ã®çµæœã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
    keys_to_clear = ["search_results_title", "last_query_title", "metadata_title", "is_ja_q_title"]
    for k in keys_to_clear:
        if k in st.session_state:
            del st.session_state[k]
    if "query_input_title" in st.session_state:
        st.session_state.query_input_title = ""

def _clear_text_tab_results():
    """ã‚¿ãƒ–ã€Œæœ¬æ–‡æ¤œç´¢ã€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
    keys_to_clear = ["search_results", "last_query", "metadata", "translations", "is_last_query_ja"]
    for k in keys_to_clear:
        if k in st.session_state:
            del st.session_state[k]
    if "query_input_text" in st.session_state:
        st.session_state.query_input_text = ""

def _clear_analysis_tab_results():
    """ã‚¿ãƒ–ã€Œæœ¬æ–‡æ¤œç´¢ã€ã®åˆ†æçµæœã‚’ã‚¯ãƒªã‚¢ã™ã‚‹"""
    keys_to_clear = ["segmented_sentences"]
    for k in keys_to_clear:
        if k in st.session_state:
            del st.session_state[k]
    if "analysis_input" in st.session_state:
        st.session_state.analysis_input = ""

def mask_list_markers(text: str) -> str:
    """pysbdãŒèª¤èªè­˜ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç®‡æ¡æ›¸ããƒãƒ¼ã‚«ãƒ¼ã‚’ä¸€æ™‚çš„ã«ç½®æ›ã™ã‚‹"""
    replacements = {
        '(a)': '__PAREN_A__', '(b)': '__PAREN_B__', '(c)': '__PAREN_C__',
        '(d)': '__PAREN_D__', '(e)': '__PAREN_E__', '(f)': '__PAREN_F__',
        '(i)': '__PAREN_I__', '(ii)': '__PAREN_II__', '(iii)': '__PAREN_III__',
        '(iv)': '__PAREN_IV__',
    }
    for old, new in replacements.items():
        text = re.sub(r'(\s|^)' + re.escape(old) + r'(\s|$|\,)', r'\1' + new + r'\2', text)
    return text

def unmask_list_markers(text: str) -> str:
    """ãƒã‚¹ã‚­ãƒ³ã‚°ã—ãŸæ–‡å­—åˆ—ã‚’å…ƒã«æˆ»ã™"""
    replacements = {
        '__PAREN_A__': '(a)', '__PAREN_B__': '(b)', '__PAREN_C__': '(c)',
        '__PAREN_D__': '(d)', '__PAREN_E__': '(e)', '__PAREN_F__': '(f)',
        '__PAREN_I__': '(i)', '__PAREN_II__': '(ii)', '__PAREN_III__': '(iii)',
        '__PAREN_IV__': '(iv)',
    }
    for new, old in replacements.items():
        text = text.replace(new, old)
    return text

def _escape_html(s: str) -> str:
    return html.escape(s, quote=False)

def merge_server_highlights(full_text: str, highlight_snippets: list[str]) -> str:
    if not full_text or not highlight_snippets:
        return _escape_html(full_text)
    text = _escape_html(full_text)
    em_pat = re.compile(r"<em>(.+?)</em>")
    hits = sorted(set(m.group(1) for snip in highlight_snippets for m in em_pat.finditer(snip)), key=len, reverse=True)
    for h in hits:
        if h.strip():
            text = text.replace(_escape_html(h), f"<em>{_escape_html(h)}</em>")
    return text

def client_side_highlight(full_text: str, query: str) -> str:
    if not full_text or not query:
        return _escape_html(full_text)
    text = _escape_html(full_text)
    clean_query = query.strip().strip('"')
    q_esc = _escape_html(clean_query)
    if q_esc:
        pattern = re.compile(re.escape(q_esc), re.IGNORECASE)
        text = pattern.sub(lambda m: f"<em>{m.group(0)}</em>", text)
    return text

def evaluate_translation(original_text: str, translated_text: str) -> float:
    """ç¿»è¨³ã®å“è³ªã‚’è©•ä¾¡ã—ã€0.0ã‹ã‚‰1.0ã®ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ã€‚"""
    system_prompt = "ã‚ãªãŸã¯ã€ç¿»è¨³å“è³ªã‚’å³æ ¼ã«è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæŒ‡ç¤ºã«å¾“ã„ã€è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    user_prompt = f"""ä»¥ä¸‹ã®è‹±èªåŸæ–‡ã¨æ—¥æœ¬èªè¨³ã‚’æ¯”è¼ƒã—ã€ç¿»è¨³ã®å“è³ªã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

## è©•ä¾¡åŸºæº–
- **æ­£ç¢ºæ€§**: èª¤è¨³ãŒãªãã€åŸæ–‡ã®æ„å›³ãŒæ­£ã—ãä¼ã‚ã£ã¦ã„ã‚‹ã‹ã€‚
- **å®Œå…¨æ€§**: ç¿»è¨³æ¼ã‚Œï¼ˆå˜èªã€ãƒ•ãƒ¬ãƒ¼ã‚ºã€æ–‡ï¼‰ãŒãªã„ã‹ã€‚
- **è‡ªç„¶ã•**: æ—¥æœ¬èªã¨ã—ã¦ä¸è‡ªç„¶ãªè¡¨ç¾ãŒãªã„ã‹ã€‚

## å‡ºåŠ›å½¢å¼
è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ `Score: <0.0ã‹ã‚‰1.0ã¾ã§ã®æ•°å€¤>` ã®å½¢å¼ã§ã€æ•°å€¤ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä¾‹: `Score: 0.95`

---

## è©•ä¾¡å¯¾è±¡
<original_en>{original_text}</original_en>
<translation_jp>{translated_text}</translation_jp>

## ã‚ãªãŸã®è©•ä¾¡
<answer>
Score:
"""
    try:
        response = aoai.chat.completions.create(
            model=GPT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0,
            max_tokens=10,
        )
        result_text = response.choices[0].message.content or ""
        match = re.search(r"([0-9.]+)", result_text)
        if match:
            return float(match.group(1))
        st.warning(f"ç¿»è¨³ã‚¹ã‚³ã‚¢ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹: '{result_text}'")
        return 0.0
    except Exception as e:
        st.error(f"ç¿»è¨³è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 0.0

def _get_single_translation(text_to_translate: str, context_english: str, context_japanese: str, glossary: dict, previous_translation: str = None) -> str:
    """æŒ‡å®šã•ã‚ŒãŸè‹±æ–‡ã‚’ç¿»è¨³ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚ç”¨èªé›†ã®æŒ‡ç¤ºã‚’è¿½åŠ ã€‚"""
    system_prompt = "ã‚ãªãŸã¯ã€å¤–å‹™çœã®å„ªç§€ãªç¿»è¨³å®˜ã§ã™ã€‚æ¡ç´„ã®ã‚ˆã†ãªã€æ³•çš„æ‹˜æŸåŠ›ã‚’æŒã¤å³æ ¼ãªæ–‡æ›¸ã®ç¿»è¨³ã‚’å°‚é–€ã¨ã—ã¦ã„ã¾ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæŒ‡ç¤ºã«ä¸€å­—ä¸€å¥æ­£ç¢ºã«å¾“ã£ã¦ãã ã•ã„ã€‚"
    
    # ç”¨èªé›†ã®æŒ‡ç¤ºã‚’ä½œæˆ
    glossary_instruction = ""
    if glossary:
        glossary_items = "\n".join([f"- `{en}` -> `{ja}`" for en, ja in glossary.items()])
        glossary_instruction = f"""## ç”¨èªé›†ï¼ˆæœ€å„ªå…ˆï¼‰
ä¸‹è¨˜ã®ç”¨èªé›†ã‚’**å¿…ãš**ä½¿ç”¨ã—ã€æŒ‡å®šé€šã‚Šã®æ—¥æœ¬èªè¨³ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚
{glossary_items}

"""
    
    instruction = f"""{glossary_instruction}1.  ä¸‹è¨˜ã®å„ä¾‹æ–‡ã‚’**æœ€å„ªå…ˆã®æ¨¡ç¯„**ã¨ã—ã€ãã®æ§‹é€ ã¨æ›¸å¼ã‚’å³å¯†ã«æ¨¡å€£ã—ã¦ãã ã•ã„ã€‚
2.  **æœ€é‡è¦**: è¤‡é›‘ãªä¿®é£¾èªå¥ãŒã©ã®åè©ã«ä¿‚ã‚‹ã®ã‹ï¼ˆä¿‚ã‚Šå—ã‘ï¼‰ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ãã ã•ã„ã€‚
3.  **æ›¸å¼ãƒ«ãƒ¼ãƒ«**:
    - **æ—¥ä»˜**: `2005å¹´`ã¯`äºŒåƒã€‡äº”å¹´`ã§ã¯ãªã`äºŒåƒäº”å¹´`ã®ã‚ˆã†ã«ã€å…¬æ–‡æ›¸ã¨ã—ã¦ä¸€èˆ¬çš„ãªæ¼¢æ•°å­—ã§è¡¨è¨˜ã—ã¦ãã ã•ã„ã€‚`2000å¹´`ã¯`äºŒåƒå¹´`ã¨ã—ã¾ã™ã€‚
    - **é‡‘é¡**: `ä¸ƒåä¹å„„...å††ï¼ˆä¸ƒã€ä¹ä¸‰ä¸‰ã€...å††ï¼‰`ã®ã‚ˆã†ã«ã€**ä½å–ã‚Šã‚’å«ã‚“ã æ¼¢æ•°å­—**ã¨ã€æ‹¬å¼§æ›¸ãã§**ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—**ã‚’å¿…ãšä½µè¨˜ã—ã¦ãã ã•ã„ã€‚
4.  å‚ç…§æƒ…å ±ï¼ˆè‹±èªåŸæ–‡ã¨ç¾åœ¨ã®æ—¥æœ¬èªè¨³ï¼‰ã®æ–‡ä½“ã‚„ç”¨èªã‚’æœ€å¤§é™ã«å°Šé‡ã—ã€`<translate_this>`å†…ã®è‹±æ–‡ã®ã¿ã‚’ç¿»è¨³ã—ã¾ã™ã€‚
5.  æœ€çµ‚çš„ãªç¿»è¨³çµæœã®**æœ¬æ–‡ã®ã¿**ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è§£èª¬ã‚„`<answer>`ã®ã‚ˆã†ãªã‚¿ã‚°ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""
    
    if previous_translation:
        instruction = f"""ä»¥å‰ã®ç¿»è¨³ã«ã¯èª¤ã‚ŠãŒã‚ã‚Šã¾ã—ãŸã€‚
<previous_bad_translation>{previous_translation}</previous_bad_translation>
ã“ã®èª¤ã‚Šã‚’å³å¯†ã«ä¿®æ­£ã—ã€ä¸‹è¨˜ã®æŒ‡ç¤ºã¨ä¾‹æ–‡ã«åˆè‡´ã™ã‚‹ã€ã‚ˆã‚Šæ­£ç¢ºã§è‡ªç„¶ãªç¿»è¨³ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n""" + instruction
    
    syntax_example_en = "RECOGNISING the previous activities carried out by the GIF under the Framework Agreement for International Collaboration on Research and Development of Generation IV Nuclear Energy Systems, done at Washington on 28 February 2005, as extended by the Agreement to Extend the Framework Agreement, which entered into force on 26 February 2015 (hereinafter referred to as the â€˜2005 GIF Framework Agreementâ€™), which expires on 28 February 2025..."
    syntax_example_jp = "äºŒåƒäºŒåäº”å¹´äºŒæœˆäºŒåå…«æ—¥ã«æœŸé–“æº€äº†ã™ã‚‹ã€äºŒåƒäº”å¹´äºŒæœˆäºŒåå…«æ—¥ã«ãƒ¯ã‚·ãƒ³ãƒˆãƒ³ã§ä½œæˆã•ã‚ŒãŸç¬¬4ä¸–ä»£åŸå­åŠ›ã‚·ã‚¹ãƒ†ãƒ ã®ç ”ç©¶é–‹ç™ºã«é–¢ã™ã‚‹å›½éš›å”åŠ›ã®ãŸã‚ã®æ çµ„ã¿å”å®šã§ã‚ã£ã¦ã€äºŒåƒåäº”å¹´äºŒæœˆäºŒåå…­æ—¥ã«ç™ºåŠ¹ã—ãŸåŒå”å®šã‚’å»¶é•·ã™ã‚‹å”å®šã«ã‚ˆã‚Šå»¶é•·ã•ã‚ŒãŸã‚‚ã®ï¼ˆä»¥ä¸‹ã€ŒäºŒåƒäº”å¹´GIFæ çµ„ã¿å”å®šã€ã¨ã„ã†ã€‚ï¼‰ã®ä¸‹ã§GIFãŒå®Ÿæ–½ã—ãŸã“ã‚Œã¾ã§ã®æ´»å‹•...ã‚’èªè­˜ã—ã€"
    format_example_en = "The total amount of the Debts will be five hundred and thirty-eight million nine hundred and seven thousand one hundred and forty-two yen (\\7,933,321,265) on December 8, 2025."
    format_example_jp = "å‚µå‹™ã®ç·é¡ã¯ã€äºŒåƒäºŒåäº”å¹´åäºŒæœˆå…«æ—¥ã«ã€ä¸ƒåä¹å„„ä¸‰åƒä¸‰ç™¾ä¸‰åäºŒä¸‡åƒäºŒç™¾å…­åäº”å††ï¼ˆä¸ƒã€ä¹ä¸‰ä¸‰ã€ä¸‰äºŒä¸€ã€äºŒå…­äº”å††ï¼‰ã«ãªã‚‹ã€‚"
    user_prompt = f"""ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸå‚ç…§æƒ…å ±ã«åŸºã¥ãã€æŒ‡å®šã•ã‚ŒãŸè‹±æ–‡ã‚’ç¿»è¨³ã™ã‚‹ä»»å‹™ã‚’è² ã£ã¦ã„ã¾ã™ã€‚
## æŒ‡ç¤º
{instruction}
---
## ä¾‹æ–‡1: è¤‡é›‘ãªæ§‹æ–‡
<example>
    <context_en>{syntax_example_en}</context_en>
    <context_jp>{syntax_example_jp}</context_jp>
    <translate_this>RECOGNISING the Framework Agreement, done at Washington on 28 February 2005, which expires on 28 February 2025</translate_this>
    <answer>äºŒåƒäºŒåäº”å¹´äºŒæœˆäºŒåå…«æ—¥ã«æœŸé–“æº€äº†ã™ã‚‹ã€äºŒåƒäº”å¹´äºŒæœˆäºŒåå…«æ—¥ã«ãƒ¯ã‚·ãƒ³ãƒˆãƒ³ã§ä½œæˆã•ã‚ŒãŸæ çµ„ã¿å”å®šã‚’èªè­˜ã—ã€</answer>
</example>
## ä¾‹æ–‡2: æ—¥ä»˜ã¨é‡‘é¡ã®æ›¸å¼
<example>
    <context_en>{format_example_en}</context_en>
    <context_jp>{format_example_jp}</context_jp>
    <translate_this>The total amount will be five hundred and thirty-eight million yen (\\538,000,000).</translate_this>
    <answer>å‚µå‹™ã®ç·é¡ã¯ã€äº”å„„ä¸‰åƒå…«ç™¾ä¸‡å††ï¼ˆäº”ä¸‰å…«ã€ã€‡ã€‡ã€‡ã€ã€‡ã€‡ã€‡å††ï¼‰ã«ãªã‚‹ã€‚</answer>
</example>
---
## ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ (Your Task)
<task>
    <context_en>{context_english}</context_en>
    <context_jp>{context_japanese}</context_jp>
    <translate_this>{text_to_translate}</translate_this>
    <answer>
</task>
"""
    try:
        response = aoai.chat.completions.create(model=GPT_MODEL, messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=0.0, stop=["</answer>"])
        return (response.choices[0].message.content or "ç¿»è¨³çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚").strip()
    except Exception as e:
        return f"ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@st.cache_data(show_spinner=False)
def get_translation_with_retry(text_to_translate: str, context_english: str, context_japanese: str, glossary: dict) -> tuple[str, float]:
    MAX_RETRIES, SCORE_THRESHOLD, best_translation, best_score = 3, 0.9, "", -1.0
    if not GPT_MODEL: 
        return "ç¿»è¨³æ©Ÿèƒ½ã«å¿…è¦ãªGPTãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤åãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", 0.0

    # é©ç”¨ã™ã‚‹ç”¨èªé›†ãŒã‚ã‚Œã°UIã«è¡¨ç¤º
    if glossary:
        st.info(f"é¸æŠã•ã‚ŒãŸç”¨èªã‚’é©ç”¨ã—ã¾ã™: {glossary}")

    for i in range(MAX_RETRIES):
        with st.spinner(f"ç¿»è¨³ã‚’ç”Ÿæˆä¸­... (è©¦è¡Œ {i+1}/{MAX_RETRIES})"):
            # _get_single_translation ã« UIã‹ã‚‰æ¸¡ã•ã‚ŒãŸ glossary ã‚’æ¸¡ã™
            current_translation = _get_single_translation(text_to_translate, context_english, context_japanese, glossary, best_translation if i > 0 else None)
            if "ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ" in current_translation: 
                return current_translation, 0.0
            current_score = evaluate_translation(text_to_translate, current_translation)
            st.write(f"è©¦è¡Œ {i+1}: ã‚¹ã‚³ã‚¢ = {current_score:.2f}, ç¿»è¨³ = '{current_translation}'")
            if current_score > best_score:
                best_score, best_translation = current_score, current_translation
            if best_score >= SCORE_THRESHOLD:
                st.write(f"å“è³ªåŸºæº– ({SCORE_THRESHOLD}) ã‚’æº€ãŸã—ã¾ã—ãŸã€‚")
                break
    return best_translation, best_score

def perform_search(query_text: str, enable_title_search: bool, mode_override: str = None, match_type_override: str = None) -> tuple[list, str]:
    """
    Azure Search ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    mode_override ã¨ match_type_override ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€UIã®è¨­å®šã‚’ç„¡è¦–ã—ã¦æ¤œç´¢æ–¹æ³•ã‚’å¼·åˆ¶ã§ãã‚‹ã€‚
    """
    t0 = time.perf_counter()
    
    # å¼•æ•°ã§æŒ‡å®šãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã„ã€ãªã‘ã‚Œã°UIï¼ˆsession_stateï¼‰ã‹ã‚‰è¨­å®šã‚’å–å¾—
    mode_now = mode_override if mode_override is not None else st.session_state.get("mode_radio", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ–‡å­—åˆ—æ¤œç´¢ + ã‚ã„ã¾ã„æ¤œç´¢)")
    match_type_now = match_type_override if match_type_override is not None else st.session_state.get("match_type_radio", "éƒ¨åˆ†ä¸€è‡´ (OR)")
    
    lang_mode = st.session_state.get("lang_mode_radio", "è¨€èªè‡ªå‹•åˆ¤å®š")
    
    is_title_search = enable_title_search

    if is_title_search:
        is_ja_q, text_fields, vec_field, mode_now = True, ["jp_title"], "japaneseVector", "æ–‡å­—åˆ—æ¤œç´¢ã®ã¿"
    else:
        is_ja_q = is_japanese(query_text) if lang_mode == "è¨€èªè‡ªå‹•åˆ¤å®š" else (lang_mode == "æ—¥æœ¬èª")
        text_fields, vec_field = (["jp_text"], "japaneseVector") if is_ja_q else (["en_text"], "englishVector")
    
    # ã€Œå®Œå…¨ä¸€è‡´ã€ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ã€ã‚¯ã‚¨ãƒªã‚’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã‚€
    search_q = f'"{query_text.strip()}"' if match_type_now == "å®Œå…¨ä¸€è‡´ (Phrase)" else query_text.strip()
    
    common_kwargs = dict(select=["en_text", "jp_text", "sourceFile", "line_number", "jp_title", "valid_date"], top=st.session_state.get("topk_slider", 10), include_total_count=True)

    odata_filters = []
    if st.session_state.get("date_filter_enabled", False):
        start_date = st.session_state.get("start_date")
        end_date = st.session_state.get("end_date")
        
        # æ—¥ä»˜ã®å¦¥å½“æ€§ã‚’å†åº¦ãƒã‚§ãƒƒã‚¯
        if start_date and end_date and start_date <= end_date:
            # Azure Searchã¯DateTimeOffsetå½¢å¼ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€ISO 8601å½¢å¼ã®UTCã§æŒ‡å®š
            # é–‹å§‹æ—¥ã¯ãã®æ—¥ã®å§‹ã¾ã‚Š (00:00:00Z)
            start_date_str = datetime.combine(start_date, datetime.min.time()).isoformat() + "Z"
            # çµ‚äº†æ—¥ã¯ãã®æ—¥ã®çµ‚ã‚ã‚Š (23:59:59Z)
            end_date_str = datetime.combine(end_date, datetime.max.time()).isoformat() + "Z"
            
            date_filter_query = f"valid_date ge {start_date_str} and valid_date le {end_date_str}"
            odata_filters.append(date_filter_query)
    
    if odata_filters:
        common_kwargs['filter'] = " and ".join(odata_filters)

    if is_title_search: 
        common_kwargs["order_by"] = "line_number asc"
    
    search_args = {}
    if mode_now in ["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ–‡å­—åˆ—æ¤œç´¢ + ã‚ã„ã¾ã„æ¤œç´¢)", "æ–‡å­—åˆ—æ¤œç´¢ã®ã¿"]:
        search_args.update({'search_text': search_q, 'search_fields': text_fields, 'highlight_fields': ",".join(text_fields), 'highlight_pre_tag': "<em>", 'highlight_post_tag': "</em>"})
    
    if mode_now in ["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ–‡å­—åˆ—æ¤œç´¢ + ã‚ã„ã¾ã„æ¤œç´¢)", "ã‚ã„ã¾ã„æ¤œç´¢ã®ã¿"]:
        try:
            emb = aoai.embeddings.create(model=EMBED_MODEL, input=[query_text]).data[0].embedding
            search_args['vector_queries'] = [VectorizedQuery(vector=emb, fields=vec_field, k_nearest_neighbors=st.session_state.get("kvec_slider", 30))]
        except Exception as e: st.warning(f"Embeddingã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    if not search_args:
        st.error("æ¤œç´¢å¼•æ•°ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return [], ""
    
    results = search.search(**common_kwargs, **search_args)
    result_list = list(results)
    
    # è¡¨ç¤ºç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    display_match_type = "å®Œå…¨ä¸€è‡´" if match_type_now == "å®Œå…¨ä¸€è‡´ (Phrase)" else "éƒ¨åˆ†ä¸€è‡´"
    metadata = f"æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: **{mode_now} ({display_match_type})** | è¨€èª: {lang_mode} | Top={common_kwargs['top']} | Time: {(time.perf_counter() - t0) * 1000:.1f} ms | Hits: {results.get_count()}"
    
    st.session_state.is_last_query_ja = is_ja_q
    return result_list, metadata

@st.cache_data(show_spinner="æ¡ç´„å…¨æ–‡ã‚’å–å¾—ä¸­...")
def fetch_full_treaty_text(source_file: str) -> list:
    """æŒ‡å®šã•ã‚ŒãŸsourceFileã®å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’è¡Œç•ªå·é †ã«å–å¾—ã™ã‚‹"""
    try:
        escaped_source_file = source_file.replace("'", "''")
        odata_filter = f"sourceFile eq '{escaped_source_file}'"
        results = search.search(
            search_text="*",
            filter=odata_filter,
            order_by=["line_number asc"],
            select=["en_text", "jp_text", "line_number"],
            top=2000
        )
        return list(results)
    except Exception as e:
        st.error(f"æ¡ç´„å…¨æ–‡ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

# ==============================================================================
# ---------- UIæç”»ãƒ­ã‚¸ãƒƒã‚¯ ----------
# ==============================================================================

def display_analysis_page(text_to_analyze: str):
    """åŸæ–‡è§£æå°‚ç”¨ãƒšãƒ¼ã‚¸ã®æç”»"""
    st.title("âš™ï¸åŸæ–‡ã®ä¿‚ã‚Šå—ã‘è§£æ")
    st.markdown("---")
    st.markdown(f"**è§£æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ:**")
    st.markdown(f"> {text_to_analyze.replace(chr(10), chr(10) + '> ')}")
    st.markdown("---")
    if not stanza_nlp:
        st.error("Stanzaãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    try:
        doc = stanza_nlp(text_to_analyze)
        dot = graphviz.Digraph(graph_attr={'rankdir': 'TB'}, node_attr={'shape': 'record', 'fontname': 'sans-serif'}, edge_attr={'fontsize': '10', 'fontname': 'sans-serif'})
        for sent in doc.sentences:
            for word in sent.words:
                node_id = f"{sent.index+1}_{word.id}"
                pos_ja = pos_tag_japanese.get(word.upos, word.upos)
                dot.node(node_id, label=f"{{{word.text}|{pos_ja}}}")
            for word in sent.words:
                if word.head > 0:
                    head_id = f"{sent.index+1}_{word.head}"
                    node_id = f"{sent.index+1}_{word.id}"
                    deprel_ja = deprel_japanese.get(word.deprel, word.deprel)
                    dot.edge(head_id, node_id, label=deprel_ja)
        st.write("### è§£æçµæœ")
        st.graphviz_chart(dot)
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def display_full_treaty_page(treaty_id: str):
    """å…¨æ–‡è¡¨ç¤ºå°‚ç”¨ãƒšãƒ¼ã‚¸ã®æç”»"""
    treaty_title = ""
    try:
        escaped_treaty_id = treaty_id.replace("'", "''")
        odata_filter = f"sourceFile eq '{escaped_treaty_id}'"
        results = search.search(
            search_text="*",
            filter=odata_filter,
            select=["jp_title"],
            top=1
        )
        if first_result := next(results, None):
            treaty_title = first_result.get("jp_title", "")
    except Exception as e:
        st.warning(f"æ¡ç´„åã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.title(treaty_title or "æ¡ç´„å…¨æ–‡")
    st.subheader(treaty_id.replace(".csv", ".pdf"))
    if full_treaty_chunks := fetch_full_treaty_text(treaty_id):
        full_en_text = "\n\n".join([c.get("en_text", "") for c in full_treaty_chunks])
        full_ja_text = "\n\n".join([c.get("jp_text", "") for c in full_treaty_chunks])
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("æ—¥æœ¬èªå…¨æ–‡")
            st.text_area("Japanese Full Text", full_ja_text, height=800, key="modal_ja_text", label_visibility="collapsed")
        with col2:
            st.subheader("è‹±èªå…¨æ–‡")
            st.text_area("English Full Text", full_en_text, height=800, key="modal_en_text", label_visibility="collapsed")
    else:
        st.error("å…¨æ–‡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

def display_maintenance_page():
    """è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®ç·¨é›†ãƒšãƒ¼ã‚¸ã®æç”»ã¨æ©Ÿèƒ½"""
    st.subheader("è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®ç·¨é›†")
    st.info("ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç›´æ¥ç·¨é›†ã—ã€ã€Œå¤‰æ›´ã‚’ä¿å­˜ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚è¡Œã®è¿½åŠ ãƒ»å‰Šé™¤ã‚‚å¯èƒ½ã§ã™ã€‚")
    
    conn = init_db()
    
    # --- ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§è¡¨ç¤º ---
    try:
        # st.data_editorã§æ‰±ã„ã‚„ã™ã„ã‚ˆã†ã«Pandas DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€
        db_df = pd.read_sql_query("SELECT id, english_term, japanese_term FROM terms ORDER BY english_term", conn)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç·¨é›†ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ‡ã‚£ã‚¿ã‚’è¡¨ç¤º
        edited_df = st.data_editor(
            db_df,
            column_config={
                "id": st.column_config.NumberColumn("ID", disabled=True), # IDã¯ç·¨é›†ä¸å¯ã«ã™ã‚‹
                "english_term": "è‹±èªåŸæ–‡",
                "japanese_term": "æ—¥æœ¬èªè¨³",
            },
            num_rows="dynamic", # è¡Œã®è¿½åŠ ã¨å‰Šé™¤ã‚’æœ‰åŠ¹ã«ã™ã‚‹
            key="glossary_editor",
            width='stretch'
        )

        if st.button("å¤‰æ›´ã‚’ä¿å­˜ ğŸ’¾", type="primary"):
            cursor = conn.cursor()
            
            # --- å¤‰æ›´å‰å¾Œã®å·®åˆ†ã‚’è¦‹ã¤ã‘ã¦DBã«åæ˜  ---
            orig_ids = set(db_df['id'])
            edited_ids = set(edited_df['id'].dropna()) # æ–°è¦è¡Œã¯IDãŒNaNãªã®ã§é™¤å¤–
            
            # 1. å‰Šé™¤ã•ã‚ŒãŸè¡Œã‚’ç‰¹å®š
            deleted_ids = orig_ids - edited_ids
            if deleted_ids:
                cursor.executemany("DELETE FROM terms WHERE id = ?", [(id,) for id in deleted_ids])
                st.write(f"ğŸ—‘ï¸ {len(deleted_ids)}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")

            # 2. è¿½åŠ ã•ã‚ŒãŸè¡Œã‚’ç‰¹å®š
            new_rows = edited_df[edited_df['id'].isna()]
            if not new_rows.empty:
                insert_data = [
                    (row['english_term'], row['japanese_term'])
                    for _, row in new_rows.iterrows()
                    if pd.notna(row['english_term']) and pd.notna(row['japanese_term']) # ç©ºã®è¡Œã¯ç„¡è¦–
                ]
                if insert_data:
                    cursor.executemany(
                        "INSERT INTO terms (english_term, japanese_term) VALUES (?, ?)",
                        insert_data
                    )
                    st.write(f"âœ¨ {len(insert_data)}ä»¶ã®ç”¨èªã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

            # 3. å¤‰æ›´ã•ã‚ŒãŸè¡Œã‚’ç‰¹å®š
            # å¤‰æ›´å‰ã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ã—ã¦ã€å€¤ãŒç•°ãªã‚‹è¡Œã‚’è¦‹ã¤ã‘ã‚‹
            comparison_df = pd.merge(db_df, edited_df, on='id', how='inner', suffixes=('_orig', '_new'))

            # å¤‰æ›´ãŒã‚ã£ãŸã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–
            update_mask = pd.Series([False] * len(comparison_df))

            # 'english_term' ã‚«ãƒ©ãƒ ã®å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯
            if 'english_term_orig' in comparison_df.columns and 'english_term_new' in comparison_df.columns:
                update_mask |= (comparison_df['english_term_orig'] != comparison_df['english_term_new'])

            # 'japanese_term' ã‚«ãƒ©ãƒ ã®å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯
            if 'japanese_term_orig' in comparison_df.columns and 'japanese_term_new' in comparison_df.columns:
                update_mask |= (comparison_df['japanese_term_orig'] != comparison_df['japanese_term_new'])

            updated_rows = comparison_df[update_mask]            
            if not updated_rows.empty:
                update_data = [
                    (row['english_term_new'], row['japanese_term_new'], row['id'])
                    for _, row in updated_rows.iterrows()
                ]
                cursor.executemany(
                    "UPDATE terms SET english_term = ?, japanese_term = ? WHERE id = ?",
                    update_data
                )
                st.write(f"âœï¸ {len(update_data)}ä»¶ã®ç”¨èªã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

            conn.commit()
            load_all_terms.clear()
            st.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¤‰æ›´ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
            st.rerun() # ç”»é¢ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦æœ€æ–°ã®çŠ¶æ…‹ã‚’è¡¨ç¤º

    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.warning("`glossary.db`ã«`terms`ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ã€`id`, `english_term`, `japanese_term`ã‚«ãƒ©ãƒ ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def display_check_page(text_to_check: str):
    """å¹³ä»„ç¢ºèªç”¨ã®å°‚ç”¨ãƒšãƒ¼ã‚¸ã‚’æç”»"""
    st.title("ğŸ“ å¹³ä»„ç¢ºèªå‡¦ç†")
    st.info("AIã«ã‚ˆã‚‹ç¿»è¨³çµæœã‚’ç·¨é›†ã§ãã¾ã™ã€‚")
    
    edited_text = st.text_area(
        "ç·¨é›†å¯èƒ½ãªç¿»è¨³æ–‡",
        value=text_to_check,
        height=400,
        label_visibility="collapsed"
    )
    # ä»Šå¾Œã®æ©Ÿèƒ½æ‹¡å¼µã®ãŸã‚ã«ã€ç·¨é›†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã¯ edited_text å¤‰æ•°ã§å—ã‘å–ã£ã¦ãŠãã¾ã™ã€‚

def display_term_search_results_page(term: str):
    """ç”¨èªã®å®Œå…¨ä¸€è‡´æ¤œç´¢ã®çµæœã‚’å°‚ç”¨ãƒšãƒ¼ã‚¸ã«è¡¨ç¤ºã™ã‚‹"""
    st.title(f"ğŸ” ç”¨èªæ¤œç´¢çµæœ: \"{term}\"")
    st.info("æŒ‡å®šã•ã‚ŒãŸç”¨èªã§ã®å®Œå…¨ä¸€è‡´æ¤œç´¢çµæœã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

    # æ—¢å­˜ã®æ¤œç´¢é–¢æ•°ã‚’ã€Œå®Œå…¨ä¸€è‡´ã€ã§å®Ÿè¡Œ
    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§å›²ã‚€ã¨å®Œå…¨ä¸€è‡´æ¤œç´¢ã«ãªã‚‹
    results, metadata = perform_search(
        query_text=term, 
        enable_title_search=False,
        mode_override="æ–‡å­—åˆ—æ¤œç´¢ã®ã¿",
        match_type_override="å®Œå…¨ä¸€è‡´ (Phrase)"
    )

    st.caption(metadata)
    st.divider()

    if not results:
        st.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # é¡ä¼¼æ–‡æ¤œç´¢çµæœã¨åŒã˜ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§çµæœã‚’è¡¨ç¤º
    for result_item in results:
        res_en, res_ja, source_file = result_item.get("en_text", ""), result_item.get("jp_text", ""), result_item.get("sourceFile", "")
        jp_title = result_item.get("jp_title", "")
        valid_date = result_item.get("valid_date", "") # valid_date ã‚’å–å¾—
        source_file_display = source_file.replace(".csv", ".pdf")
        
        title_prefix = f"**{jp_title}**" if jp_title else ""
        valid_date_str = result_item.get("valid_date", "")
        date_display = ""
        if valid_date_str:
            try:
                # ISOå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã€æŒ‡å®šã®æ›¸å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
            except (ValueError, TypeError):
                # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾è¡¨ç¤º
                date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"

        metadata_str = f"{title_prefix}{date_display} | Source: **{source_file_display}#{result_item['line_number']}** | Score: {result_item['@search.score']:.4f}"

        res_col1, res_col2 = st.columns([0.8, 0.2])
        with res_col1: st.markdown(metadata_str)
        with res_col2: st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’é–‹ã</a>', unsafe_allow_html=True)
        
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†
        if is_japanese(term):
            en_html_highlighted = _escape_html(res_en)
            ja_html_highlighted = client_side_highlight(res_ja, term)
        else:
            en_html_highlighted = client_side_highlight(res_en, term)
            ja_html_highlighted = _escape_html(res_ja)        
        st.markdown(f"**è‹±èªåŸæ–‡:**<br>{en_html_highlighted}", unsafe_allow_html=True)
        st.markdown(f"**æ—¥æœ¬èªè¨³:**<br>{ja_html_highlighted}", unsafe_allow_html=True)
        st.divider()

def display_search_interface():
    """ãƒ¡ã‚¤ãƒ³ã®æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æç”»"""
    db_conn = init_db()

    # å®‰å®šåŒ–ã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®çŠ¶æ…‹å¤‰æ•°ã‚’æœ€åˆã«åˆæœŸåŒ–
    if "query_input_title" not in st.session_state:
        st.session_state.query_input_title = ""
    if "analysis_input" not in st.session_state:
        st.session_state.analysis_input = ""

    with st.sidebar:
        st.title("æ¡ç´„æ–‡æ¤œç´¢")
        st.subheader("æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        st.radio("æ¤œç´¢è¨€èª", ["è¨€èªè‡ªå‹•åˆ¤å®š", "è‹±èª", "æ—¥æœ¬èª"], key="lang_mode_radio")
        st.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ–‡å­—åˆ—æ¤œç´¢ + ã‚ã„ã¾ã„æ¤œç´¢)", "æ–‡å­—åˆ—æ¤œç´¢ã®ã¿", "ã‚ã„ã¾ã„æ¤œç´¢ã®ã¿"], key="mode_radio")
        st.radio("ä¸€è‡´æ–¹æ³•", ["éƒ¨åˆ†ä¸€è‡´ (OR)", "å®Œå…¨ä¸€è‡´ (Phrase)"], key="match_type_radio")
        st.slider("ä¸Šä½ (è¡¨ç¤ºä»¶æ•°)", 1, 50, 10, key="topk_slider")
        st.slider("k (ã‚ã„ã¾ã„æ¤œç´¢ è¿‘æ¥åº¦)", 1, 200, 30, key="kvec_slider")

        st.divider()
        st.subheader("æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        if st.checkbox("åŠ¹åŠ›ç™ºç”Ÿæ—¥ã§çµã‚Šè¾¼ã¿", key="date_filter_enabled"):
            today = datetime.now().date()
            # 1950å¹´1æœˆ1æ—¥ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–‹å§‹æ—¥ã¨ã™ã‚‹
            default_start = datetime(1950, 1, 1).date()
            
            start_date = st.date_input(
                "é–‹å§‹æ—¥", 
                value=default_start, 
                key="start_date"
            )
            end_date = st.date_input(
                "çµ‚äº†æ—¥", 
                value=today, 
                key="end_date"
            )

            # æ—¥ä»˜ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if start_date > end_date:
                st.error("ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")

    tab_text_search, tab_title_search, tab_maintenance = st.tabs(["âœï¸ æ¡ç´„æœ¬æ–‡æ¤œç´¢", "ğŸ“œ æ¡ç´„åæ¤œç´¢", "ğŸ“– è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®ç·¨é›†"])

    with tab_title_search:
        st.subheader("æ¡ç´„åã§æ¤œç´¢")
        q_title = st.text_input("æ¤œç´¢ã—ãŸã„æ¡ç´„åï¼ˆæ—¥æœ¬èªï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="query_input_title")

        col1_title, col2_title, _ = st.columns([1, 1, 5])
        with col1_title:
            run_clicked_title = st.button("ğŸ”æ¡ç´„åæ¤œç´¢", key="search_button_title")
        with col2_title:
            st.button("ğŸ§¹å…¥åŠ›æ¶ˆå»ã€€ã€€", key="clear_button_title", on_click=_clear_title_tab_results)

        if run_clicked_title and q_title.strip():
            st.session_state.last_query_title = q_title
            try:
                results, metadata = perform_search(q_title, enable_title_search=True)
                st.session_state.search_results_title = results
                st.session_state.metadata_title = metadata
            except Exception as e:
                st.exception(e)
                st.session_state.search_results_title = None
        elif run_clicked_title and not q_title.strip():
            st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        if "search_results_title" in st.session_state and st.session_state.search_results_title is not None:
            st.divider()
            st.caption(st.session_state.metadata_title)
            results_to_display = st.session_state.search_results_title
            if not results_to_display:
                st.info("æ¤œç´¢çµæœã¯0ä»¶ã§ã—ãŸã€‚")

            displayed_files = set()
            for r in results_to_display:
                source_file = r.get("sourceFile", "")
                if source_file in displayed_files:
                    continue
                displayed_files.add(source_file)
                jp_title = r.get("jp_title", "")
                source_file_display = source_file.replace(".csv", ".pdf")
                highlights = r.get("@search.highlights", {})
                highlighted_snippets = highlights.get("jp_title", [])

                if highlighted_snippets:
                    # ãƒã‚¤ãƒ©ã‚¤ãƒˆçµæœãŒã‚ã‚Œã°ã€ãã‚Œã‚’çµåˆã—ã¦è¡¨ç¤º
                    highlighted_title = " ... ".join(highlighted_snippets)
                else:
                    # ãªã‘ã‚Œã°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                    query_to_display = st.session_state.last_query_title
                    highlighted_title = client_side_highlight(jp_title, query_to_display)                
                st.markdown(f"##### {highlighted_title}", unsafe_allow_html=True)
                res_col1, res_col2 = st.columns([0.75, 0.25])
                with res_col1:
                    valid_date_str = r.get("valid_date", "")
                    date_display = ""
                    if valid_date_str:
                        try:
                            # ISOå½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã€æŒ‡å®šã®æ›¸å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                            formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                            date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
                        except (ValueError, TypeError):
                            # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾è¡¨ç¤º
                            date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"
                    st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {source_file_display}{date_display}")
                with res_col2:
                    st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’åˆ¥ã‚¿ãƒ–ã§é–‹ã</a>', unsafe_allow_html=True)
                st.markdown("---")

    with tab_text_search:
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ãƒ»é¡ä¼¼æ¡ç´„æ–‡æ¤œç´¢")
        pasted_text = st.text_area("å¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘", height=200, key="analysis_input")

        col1_tab2, col2_tab2, col3_tab2, _, _, _ = st.columns([2, 2, 2, 3, 3, 3])
        with col1_tab2:
            start_analysis_clicked = st.button("âœ‚ï¸æ–‡ç« åˆ†å‰²ã™ã‚‹ã€€", key="start_analysis_button")

        with col2_tab2:
            no_split_clicked = st.button("ğŸ“æ–‡ç« åˆ†å‰²ã—ãªã„", key="no_split_button")
        with col3_tab2:
            st.button("ğŸ§¹å…¥åŠ›æ¶ˆå»ã€€ã€€ã€€", key="clear_button_analysis", on_click=_clear_analysis_tab_results)
        
        if start_analysis_clicked or no_split_clicked:
            if not pasted_text.strip():
                st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            # ã€Œæ–‡ç« åˆ†å‰²ã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸæ™‚ã ã‘NLPãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            elif not nlp and start_analysis_clicked:
                st.error("NLPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                st.session_state.segmented_sentences = []
                
                # ã€Œæ–‡ç« åˆ†å‰²ã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
                if start_analysis_clicked:
                    doc = nlp(mask_list_markers(pasted_text))
                    for sent in doc.sents:
                        if original_sent_text := unmask_list_markers(sent.text).strip():
                            st.session_state.segmented_sentences.append({"text": original_sent_text, "search_results": None})
                
                # ã€Œæ–‡ç« åˆ†å‰²ã—ãªã„ã€ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
                elif no_split_clicked:
                    # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’ä¸€ã¤ã®è¦ç´ ã¨ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    st.session_state.segmented_sentences.append({"text": pasted_text.strip(), "search_results": None})

        if "segmented_sentences" in st.session_state:
            st.markdown("---")
            
            num_sents = len(st.session_state.segmented_sentences)
            if num_sents > 1:
                st.write(f"â–¼ {num_sents} ä»¶ã®æ–‡ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸ â–¼")
            else:
                st.write(f"â–¼ 1 ä»¶ã®æ–‡ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ â–¼")

            for i, sentence_data in enumerate(st.session_state.segmented_sentences):
                with st.expander(f"æ–‡ {i+1}: {sentence_data['text'][:80]}..."):
                    original_text = sentence_data['text']
                    st.markdown(f"ğŸ“˜**åŸæ–‡:**\n> {original_text.replace(chr(10), chr(10) + '> ')}")

                    # --- ãƒœã‚¿ãƒ³ã‚’ä¸­å¤®ã«å¯„ã›ã‚‹ãŸã‚ã®åˆ—å®šç¾© ---
                    # [ç©ºç™½, ãƒœã‚¿ãƒ³1, ãƒœã‚¿ãƒ³2, ãƒœã‚¿ãƒ³3, ãƒœã‚¿ãƒ³4, ç©ºç™½] ã®æ¯”ç‡ã§åˆ—ã‚’ä½œæˆ
                    c1, c2, c3, c4, _, _ = st.columns([2, 2, 2, 2, 3, 3])

                    with c1: # é¡ä¼¼æ¡ç´„æ–‡æ¤œç´¢ãƒœã‚¿ãƒ³
                        if st.button("ğŸ”é¡ä¼¼æ¡ç´„æ–‡æ¤œç´¢", key=f"search_{i}"):
                            try:
                                results, metadata = perform_search(sentence_data['text'], enable_title_search=False)
                                st.session_state.segmented_sentences[i]["search_results"] = [{"checked": False, **res} for res in results]
                                if "ai_translation" in st.session_state.segmented_sentences[i]:
                                    del st.session_state.segmented_sentences[i]["ai_translation"]
                                st.rerun()
                            except Exception as e: st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

                    with c2: # ä¿‚ã‚Šå—ã‘è§£æãƒœã‚¿ãƒ³
                        is_disabled = not (original_text and not is_japanese(original_text))
                        if st.button("âš™ï¸ ä¿‚ã‚Šå—ã‘è§£æ", key=f"analysis_{i}", disabled=is_disabled, help="è§£æå¯¾è±¡ã¯è‹±èªã®æ–‡ã®ã¿ã§ã™ã€‚"):
                            url_to_open = f"?analyze_text={urllib.parse.quote(original_text)}"
                            st.components.v1.html(f"<script>window.open('{url_to_open}', '_blank');</script>", height=0)

                    with c3: # è¾æ›¸å‚ç…§ãƒœã‚¿ãƒ³
                        if st.button("ğŸ“–ç™»éŒ²è¾æ›¸å‚ç…§", key=f"glossary_{i}"):
                            found_terms_dict = find_glossary_terms(original_text, db_conn)
                            term_list_for_display = []
                            for en_term, ja_term_list in found_terms_dict.items():
                                for ja_term in ja_term_list:
                                    term_list_for_display.append(
                                        {"en": en_term, "ja": ja_term, "checked": False}
                                    )
                            st.session_state.segmented_sentences[i]["found_terms"] = term_list_for_display
                            st.rerun()

                    with c4: # å‚ç…§ã—ã¦æ—¥æœ¬èªè¨³ãƒœã‚¿ãƒ³
                        if sentence_data.get("search_results"):
                            if st.button("ğŸ”¤å‚ç…§ã—ã¦æ—¥æœ¬èªè¨³", key=f"translate_all_{i}"):
                                selected_results = [res for res in sentence_data["search_results"] if res.get("checked", False)]
                                if not selected_results:
                                    st.warning("ç¿»è¨³ã®å‚ç…§ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹è¡Œã‚’å°‘ãªãã¨ã‚‚1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚")
                                else:
                                    context_english = "\\n\\n---\\n\\n".join([r.get("en_text", "") for r in selected_results])
                                    context_japanese = "\\n\\n---\\n\\n".join([r.get("jp_text", "") for r in selected_results])
                                    glossary_to_use = {}
                                    if "found_terms" in sentence_data and sentence_data["found_terms"]:
                                        for term_data in sentence_data["found_terms"]:
                                            if term_data["checked"]:
                                                glossary_to_use[term_data["en"]] = term_data["ja"]
                                    translation, score = get_translation_with_retry(original_text, context_english, context_japanese, glossary_to_use)
                                    st.session_state.segmented_sentences[i]["ai_translation"] = {"text": translation, "score": score}
                                    st.rerun()
                        else:
                            st.button("ğŸ”¤å‚ç…§ã—ã¦æ—¥æœ¬èªè¨³", disabled=True, key=f"translate_all_{i}_disabled", help="å…ˆã«é¡ä¼¼æ–‡æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

                    # 1. AIç¿»è¨³çµæœ (å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¡¨ç¤º)
                    if "ai_translation" in sentence_data and sentence_data["ai_translation"]:
                        # ãƒ©ãƒ™ãƒ«ã‚’st.markdownã§å¤–å‡ºã—ã«ã™ã‚‹
                        st.markdown("---")
                        st.markdown("ğŸ”¤**AIç¿»è¨³çµæœ:**")
                        translation_data = sentence_data["ai_translation"]
                        # st.infoã«ã¯ç¿»è¨³ãƒ†ã‚­ã‚¹ãƒˆæœ¬ä½“ã®ã¿ã‚’è¡¨ç¤º
                        st.info(f"{translation_data['text']} (ç¿»è¨³ã‚¹ã‚³ã‚¢: {translation_data['score']:.2f})")

                        translated_text = translation_data['text']
                        if st.button("ğŸ“ å¹³ä»„ç¢ºèªå‡¦ç†", key=f"check_text_{i}"):
                            url_to_open = f"?check_text={urllib.parse.quote(translated_text)}"
                            st.components.v1.html(f"<script>window.open('{url_to_open}', '_blank');</script>", height=0)

                    # 2. é©ç”¨ã™ã‚‹è¾æ›¸ç”¨èª
                    # "found_terms"ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆ=ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå¾Œã‹ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if "found_terms" in sentence_data:
                        st.markdown("---")
                        # ç”¨èªãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                        if sentence_data["found_terms"]:
                            st.markdown("ğŸ“–**é©ç”¨ã™ã‚‹è¾æ›¸ç”¨èªã‚’é¸æŠ:**")
                            separator_html = "<div style='background-color: #ddd; height: 1px; margin: 10px 0;'></div>"
                            with st.container(border=True):
                                header_cols = st.columns([0.1, 0.45, 0.45])
                                header_cols[0].markdown("**é©ç”¨**")
                                header_cols[1].markdown("**è‹±èªåŸæ–‡**")
                                header_cols[2].markdown("**æ—¥æœ¬èªè¨³**")
                                for term_idx, term_data in enumerate(sentence_data["found_terms"]):
                                    st.markdown(separator_html, unsafe_allow_html=True)
                                    row_cols = st.columns([0.1, 0.45, 0.45], vertical_alignment="center")
                                    with row_cols[0]:
                                        is_checked = st.checkbox(" ", value=term_data["checked"], key=f"term_check_{i}_{term_idx}", label_visibility="collapsed")
                                        st.session_state.segmented_sentences[i]["found_terms"][term_idx]["checked"] = is_checked
                                    with row_cols[1]:
                                        en_term = term_data["en"]
                                        en_url = f"?search_term={urllib.parse.quote(en_term)}"
                                        st.markdown(f'<a href="{en_url}" target="_blank" style="text-decoration: none;">{en_term}</a>', unsafe_allow_html=True)
                                    with row_cols[2]:
                                        ja_term = term_data["ja"]
                                        ja_url = f"?search_term={urllib.parse.quote(ja_term)}"
                                        st.markdown(f'<a href="{ja_url}" target="_blank" style="text-decoration: none;">{ja_term}</a>', unsafe_allow_html=True)
                        else:
                            # ç”¨èªãƒªã‚¹ãƒˆãŒç©ºã®å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                            st.info("ğŸ“– è©²å½“ã™ã‚‹ç™»éŒ²è¾æ›¸ç”¨èªã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                    # 3. é¡ä¼¼æ–‡æ¤œç´¢çµæœ
                    if sentence_data.get("search_results"):
                        st.markdown("---")
                        st.markdown("ğŸ”**é¡ä¼¼æ–‡æ¤œç´¢çµæœ:**ï¼ˆç¿»è¨³ã®å‚ç…§ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹è¡Œã‚’é¸æŠã—ã¦ãã ã•ã„ï¼‰")
                        query_for_highlight = sentence_data['text']
                        is_ja_q_for_highlight = st.session_state.get("is_last_query_ja", is_japanese(query_for_highlight))

                        for j, result_item in enumerate(sentence_data["search_results"]):
                            with st.container(border=True):
                                check_col, content_col = st.columns([0.08, 0.92])

                                with check_col:
                                    is_checked = st.checkbox(
                                        " ",
                                        value=result_item.get("checked", False),
                                        key=f"res_check_{i}_{j}",
                                        label_visibility="collapsed",
                                        help="ã“ã®è¡Œã‚’ç¿»è¨³ã®å‚ç…§ã«å«ã‚ã‚‹"
                                    )
                                    st.session_state.segmented_sentences[i]["search_results"][j]["checked"] = is_checked

                                with content_col:
                                    res_en, res_ja, source_file = result_item.get("en_text", ""), result_item.get("jp_text", ""), result_item.get("sourceFile", "")
                                    highlights = result_item.get("@search.highlights") or {}
                                    en_snips, ja_snips = highlights.get("en_text", []), highlights.get("jp_text", [])
                                    jp_title_tab2 = result_item.get("jp_title", "")
                                    valid_date = result_item.get("valid_date", "") # valid_date ã‚’å–å¾—
                                    source_file_display_tab2 = source_file.replace(".csv", ".pdf")

                                    title_prefix_tab2 = f"**{jp_title_tab2}**" if jp_title_tab2 else ""

                                    valid_date_str = result_item.get("valid_date", "")
                                    date_display_tab2 = ""
                                    if valid_date_str:
                                        try:
                                            formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                                            date_display_tab2 = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
                                        except (ValueError, TypeError):
                                            date_display_tab2 = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"

                                    metadata_str = f"{title_prefix_tab2}{date_display_tab2} | Source: **{source_file_display_tab2}#{result_item['line_number']}** | Score: {result_item['@search.score']:.4f}"
 
                                    res_col1_tab2, res_col2_tab2 = st.columns([0.8, 0.2])
                                    with res_col1_tab2: st.markdown(metadata_str)
                                    with res_col2_tab2: st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’é–‹ã</a>', unsafe_allow_html=True)

                                    if is_ja_q_for_highlight:
                                        ja_html_highlighted = merge_server_highlights(res_ja, ja_snips) if ja_snips else client_side_highlight(res_ja, query_for_highlight)
                                        en_html_highlighted = _escape_html(res_en)
                                    else:
                                        en_html_highlighted = merge_server_highlights(res_en, en_snips) if en_snips else client_side_highlight(res_en, query_for_highlight)
                                        ja_html_highlighted = _escape_html(res_ja)

                                    st.markdown(f"**è‹±èªåŸæ–‡:**<br>{en_html_highlighted}", unsafe_allow_html=True)
                                    st.markdown(f"**æ—¥æœ¬èªè¨³:**<br>{ja_html_highlighted}", unsafe_allow_html=True)

    with tab_maintenance:
        display_maintenance_page()
=======
# å„ãƒšãƒ¼ã‚¸ã®æç”»é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from views.search_interface import display_search_interface
from views.full_treaty_page import display_full_treaty_page
from views.analysis_page import display_analysis_page
from views.term_search_page import display_term_search_results_page
from views.check_page import display_check_page
>>>>>>> feature/code-refactor

# ==============================================================================
# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼šè¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆ ---
# ==============================================================================
st.set_page_config(page_title="æ¡ç´„æ–‡æ¤œç´¢", layout="wide")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã§åˆ©ç”¨ã™ã‚‹CSSã‚¹ã‚¿ã‚¤ãƒ«
st.markdown("""
<style>
    /* ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºè¨­å®š */
    html, body, [class*="st-"], [class*="css-"] {
        font-size: 16px;
    }
    /* ãƒœã‚¿ãƒ³å¹…ã‚’æœ€å¤§åŒ– */
    div[data-testid="stHorizontalBlock"] div[data-testid="stButton"] > button {
        width: 100%;
    }
    /* ãƒã‚¤ãƒ©ã‚¤ãƒˆç”¨ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    em {
        background-color: #FFFF00; /* é»„è‰²ã®èƒŒæ™¯è‰² */
        font-style: normal;      /* ã‚¤ã‚¿ãƒªãƒƒã‚¯ä½“ã‚’è§£é™¤ */
    }
</style>
""", unsafe_allow_html=True)

# URLã®ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§£æ
query_params = st.query_params
treaty_id_to_display = query_params.get("view_treaty")
text_to_analyze_encoded = query_params.get("analyze_text")
term_to_search_encoded = query_params.get("search_term")
<<<<<<< HEAD
text_to_check_encoded = query_params.get("check_text") 
=======
text_to_check_encoded = query_params.get("check_text")
>>>>>>> feature/code-refactor

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¿œã˜ã¦æç”»ã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ›¿ãˆ
if treaty_id_to_display:
    display_full_treaty_page(treaty_id_to_display)
elif text_to_analyze_encoded:
    decoded_text = urllib.parse.unquote(text_to_analyze_encoded)
    display_analysis_page(decoded_text)
elif term_to_search_encoded:
    decoded_term = urllib.parse.unquote(term_to_search_encoded)
    display_term_search_results_page(decoded_term)
elif text_to_check_encoded:
    decoded_text = urllib.parse.unquote(text_to_check_encoded)
    display_check_page(decoded_text)
else:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ¡ã‚¤ãƒ³ã®æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    display_search_interface()