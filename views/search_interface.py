import streamlit as st
import urllib.parse
from datetime import datetime

# å¿…è¦ãªé–¢æ•°ã‚’å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from core.azure_clients import get_clients
from core.database import init_db, find_glossary_terms
from core.nlp import load_nlp_model
from core.search import perform_search
from core.translation import get_translation_with_retry
from utils import (
    _clear_title_tab_results,
    _clear_analysis_tab_results,
    mask_list_markers,
    unmask_list_markers,
    is_japanese,
    merge_server_highlights,
    client_side_highlight,
    _escape_html
)
from .maintenance_page import display_maintenance_page

def display_search_interface():
    """ãƒ¡ã‚¤ãƒ³ã®æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æç”»"""
    # --- åˆæœŸåŒ–å‡¦ç† ---
    search_client, aoai_client, gpt_model, embed_model = get_clients()
    db_conn = init_db()
    nlp = load_nlp_model()

    # --- UIæç”» ---
    if "query_input_title" not in st.session_state:
        st.session_state.query_input_title = ""
    if "analysis_input" not in st.session_state:
        st.session_state.analysis_input = ""

    with st.sidebar:
        st.title("æ¡ç´„æ–‡æ¤œç´¢")
        st.subheader("æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        st.radio("æ¤œç´¢è¨€èª", ["è¨€èªè‡ªå‹•åˆ¤å®š", "è‹±èª", "æ—¥æœ¬èª"], key="lang_mode_radio")
        st.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ–‡å­—åˆ—æ¤œç´¢ + ã‚ã„ã¾ã„æ¤œç´¢)", "æ–‡å­—åˆ—æ¤œç´¢ã®ã¿", "ã‚ã„ã¾ã„æ¤œç´¢ã®ã¿"], key="mode_radio")
        st.radio("ä¸€è‡´æ–¹æ³•", ["éƒ¨åˆ†ä¸€è‡´ (OR)", "å®Œå…¨ä¸€è‡´ (Phrase)"], key="match_type_radio")
        st.slider("ä¸Šä½ (è¡¨ç¤ºä»¶æ•°)", 1, 50, 10, key="topk_slider")
        st.slider("k (ã‚ã„ã¾ã„æ¤œç´¢ è¿‘æ¥åº¦)", 1, 200, 30, key="kvec_slider")

        st.divider()
        st.subheader("æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        if st.checkbox("åŠ¹åŠ›ç™ºç”Ÿæ—¥ã§çµã‚Šè¾¼ã¿", key="date_filter_enabled"):
            today = datetime.now().date()
            default_start = datetime(1950, 1, 1).date()

            start_date = st.date_input("é–‹å§‹æ—¥", value=default_start, key="start_date")
            end_date = st.date_input("çµ‚äº†æ—¥", value=today, key="end_date")

            if start_date > end_date:
                st.error("ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")

    tab_text_search, tab_title_search, tab_maintenance = st.tabs(["âœï¸ æ¡ç´„æœ¬æ–‡æ¤œç´¢", "ğŸ“œ æ¡ç´„åæ¤œç´¢", "ğŸ“– ç¿»è¨³è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã®ç·¨é›†"])

    with tab_title_search:
        st.subheader("æ¡ç´„åã§æ¤œç´¢")
        q_title = st.text_input("æ¤œç´¢ã—ãŸã„æ¡ç´„åï¼ˆæ—¥æœ¬èªï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="query_input_title")

        col1_title, col2_title, _ = st.columns([1, 1, 5])
        with col1_title:
            run_clicked_title = st.button("ğŸ”æ¡ç´„åæ¤œç´¢", key="search_button_title")
        with col2_title:
            st.button("ğŸ§¹å…¥åŠ›æ¶ˆå»ã€€", key="clear_button_title", on_click=_clear_title_tab_results)

        if run_clicked_title and q_title.strip():
            st.session_state.last_query_title = q_title
            try:
                results, metadata = perform_search(search_client, aoai_client, embed_model, q_title, enable_title_search=True)
                st.session_state.search_results_title = results
                st.session_state.metadata_title = metadata
            except Exception as e:
                st.exception(e)
                st.session_state.search_results_title = None
        elif run_clicked_title and not q_title.strip():
            st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        if "search_results_title" in st.session_state and st.session_state.search_results_title is not None:
            st.divider()
            st.caption(st.session_state.metadata_title)
            results_to_display = st.session_state.search_results_title
            if not results_to_display:
                st.info("æ¤œç´¢çµæœã¯0ä»¶ã§ã—ãŸã€‚")

            displayed_files = set()
            for r in results_to_display:
                source_file = r.get("sourceFile", "")
                if source_file in displayed_files:
                    continue
                displayed_files.add(source_file)
                jp_title = r.get("jp_title", "")
                source_file_display = source_file.replace(".csv", ".pdf")
                highlights = r.get("@search.highlights", {})
                highlighted_snippets = highlights.get("jp_title", [])

                if highlighted_snippets:
                    highlighted_title = " ... ".join(highlighted_snippets)
                else:
                    query_to_display = st.session_state.last_query_title
                    highlighted_title = client_side_highlight(jp_title, query_to_display)

                st.markdown(f"##### {highlighted_title}", unsafe_allow_html=True)
                res_col1, res_col2 = st.columns([0.75, 0.25])
                with res_col1:
                    valid_date_str = r.get("valid_date", "")
                    date_display = ""
                    if valid_date_str:
                        try:
                            formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                            date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
                        except (ValueError, TypeError):
                            date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"
                    st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {source_file_display}{date_display}")
                with res_col2:
                    st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’åˆ¥ã‚¿ãƒ–ã§é–‹ã</a>', unsafe_allow_html=True)
                st.markdown("---")

    with tab_text_search:
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ãƒ»é¡ä¼¼æ¡ç´„æ–‡æ¤œç´¢")
        pasted_text = st.text_area("å¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘", height=200, key="analysis_input")

        col1_tab2, col2_tab2, col3_tab2, _, _, _ = st.columns([2, 2, 2, 3, 3, 3])
        with col1_tab2:
            start_analysis_clicked = st.button("âœ‚ï¸æ–‡ç« åˆ†å‰²ã™ã‚‹ã€€", key="start_analysis_button")
        with col2_tab2:
            no_split_clicked = st.button("ğŸ“æ–‡ç« åˆ†å‰²ã—ãªã„", key="no_split_button")
        with col3_tab2:
            st.button("ğŸ§¹å…¥åŠ›æ¶ˆå»ã€€ã€€ã€€", key="clear_button_analysis", on_click=_clear_analysis_tab_results)

        if start_analysis_clicked or no_split_clicked:
            if not pasted_text.strip():
                st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            elif not nlp and start_analysis_clicked:
                st.error("NLPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                st.session_state.segmented_sentences = []
                if start_analysis_clicked:
                    doc = nlp(mask_list_markers(pasted_text))
                    for sent in doc.sents:
                        if original_sent_text := unmask_list_markers(sent.text).strip():
                            st.session_state.segmented_sentences.append({"text": original_sent_text, "search_results": None})
                elif no_split_clicked:
                    st.session_state.segmented_sentences.append({"text": pasted_text.strip(), "search_results": None})

        if "segmented_sentences" in st.session_state:
            st.markdown("---")
            num_sents = len(st.session_state.segmented_sentences)
            st.write(f"â–¼ {num_sents} ä»¶ã®æ–‡ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸ â–¼" if num_sents > 1 else "â–¼ 1 ä»¶ã®æ–‡ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ â–¼")

            for i, sentence_data in enumerate(st.session_state.segmented_sentences):
                # ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ç”¨ã®çŠ¶æ…‹ã‚’åˆæœŸåŒ–
                if f"fw_search_results_{i}" not in st.session_state:
                    st.session_state[f"fw_search_results_{i}"] = None
                if f"show_fw_search_{i}" not in st.session_state:
                    st.session_state[f"show_fw_search_{i}"] = False
                if f"fw_query_{i}" not in st.session_state:
                    st.session_state[f"fw_query_{i}"] = ""

                with st.expander(f"æ–‡ {i+1}: {sentence_data['text'][:80]}..."):
                    original_text = sentence_data['text']
                    st.markdown(f"ğŸ“˜**åŸæ–‡:**\n> {original_text.replace(chr(10), '  ' + chr(10) + '> ')}")

                    # ãƒœã‚¿ãƒ³ç”¨ã®åˆ—ã‚’5åˆ—ã«å¤‰æ›´
                    c1, c2, c3, c_new, c4, _ = st.columns([2, 2, 2, 2, 2, 2])
                    with c1:
                        if st.button("ğŸ”é¡ä¼¼æ¡ç´„æ–‡æ¤œç´¢", key=f"search_{i}"):
                            try:
                                results, metadata = perform_search(search_client, aoai_client, embed_model, sentence_data['text'], enable_title_search=False)
                                st.session_state.segmented_sentences[i]["search_results"] = [{"checked": False, **res} for res in results]
                                if "ai_translation" in st.session_state.segmented_sentences[i]:
                                    del st.session_state.segmented_sentences[i]["ai_translation"]
                                st.rerun()
                            except Exception as e: st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    with c2:
                        is_disabled = not (original_text and not is_japanese(original_text))
                        if st.button("âš™ï¸ ä¿‚ã‚Šå—ã‘è§£æ", key=f"analysis_{i}", disabled=is_disabled, help="è§£æå¯¾è±¡ã¯è‹±èªã®æ–‡ã®ã¿ã§ã™ã€‚"):
                            url_to_open = f"?analyze_text={urllib.parse.quote(original_text)}"
                            st.components.v1.html(f"<script>window.open('{url_to_open}', '_blank');</script>", height=0)
                    with c3:
                        if st.button("ğŸ“–ç™»éŒ²è¾æ›¸å‚ç…§", key=f"glossary_{i}"):
                            found_terms_dict = find_glossary_terms(original_text, db_conn)
                            term_list_for_display = []
                            for en_term, ja_term_list in found_terms_dict.items():
                                for ja_term in ja_term_list:
                                    term_list_for_display.append({"en": en_term, "ja": ja_term, "checked": False})
                            st.session_state.segmented_sentences[i]["found_terms"] = term_list_for_display
                            st.rerun()
                    # ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒœã‚¿ãƒ³
                    with c_new:
                        if st.button("ğŸ’¬ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", key=f"fw_search_show_{i}"):
                            st.session_state[f"show_fw_search_{i}"] = not st.session_state[f"show_fw_search_{i}"]
                            # æ¤œç´¢çª“ã‚’é–‹ãã¨ãã«ä»¥å‰ã®æ¤œç´¢çµæœã‚’ã‚¯ãƒªã‚¢
                            if st.session_state[f"show_fw_search_{i}"]:
                                st.session_state[f"fw_search_results_{i}"] = None
                                st.session_state[f"fw_query_{i}"] = ""
                            st.rerun()
                    with c4:
                        if sentence_data.get("search_results"):
                            if st.button("ğŸ”¤å‚ç…§ã—ã¦æ—¥æœ¬èªè¨³", key=f"translate_all_{i}"):
                                selected_results = [res for res in sentence_data["search_results"] if res.get("checked", False)]
                                if not selected_results:
                                    st.warning("ç¿»è¨³ã®å‚ç…§ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹è¡Œã‚’å°‘ãªãã¨ã‚‚1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚")
                                else:
                                    context_english = "\\n\\n---\\n\\n".join([r.get("en_text", "") for r in selected_results])
                                    context_japanese = "\\n\\n---\\n\\n".join([r.get("jp_text", "") for r in selected_results])
                                    glossary_to_use = {}
                                    if "found_terms" in sentence_data and sentence_data["found_terms"]:
                                        for term_data in sentence_data["found_terms"]:
                                            if term_data["checked"]:
                                                glossary_to_use[term_data["en"]] = term_data["ja"]
                                    translation, score = get_translation_with_retry(aoai_client, gpt_model, original_text, context_english, context_japanese, glossary_to_use)
                                    st.session_state.segmented_sentences[i]["ai_translation"] = {"text": translation, "score": score}
                                    st.rerun()
                        else:
                            st.button("ğŸ”¤å‚ç…§ã—ã¦æ—¥æœ¬èªè¨³", disabled=True, key=f"translate_all_{i}_disabled", help="å…ˆã«é¡ä¼¼æ–‡æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

                    # ### å¤‰æ›´ ### ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®UIã¨ãƒ­ã‚¸ãƒƒã‚¯
                    if st.session_state[f"show_fw_search_{i}"]:
                        with st.container(border=True):
                            st.markdown("##### ğŸ’¬ ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¡ç´„æœ¬æ–‡ã‚’æ¤œç´¢")
                            fw_query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", key=f"fw_query_input_{i}", value=st.session_state[f"fw_query_{i}"])
                            st.session_state[f"fw_query_{i}"] = fw_query # å…¥åŠ›ã‚’stateã«ä¿å­˜

                            fw_c1, fw_c2, fw_c3, _ = st.columns([1,1,1,4])
                            with fw_c1:
                                if st.button("æ¤œç´¢å®Ÿè¡Œ", key=f"fw_search_run_{i}"):
                                    if fw_query.strip():
                                        try:
                                            results, _ = perform_search(search_client, aoai_client, embed_model, fw_query, enable_title_search=False)
                                            st.session_state[f"fw_search_results_{i}"] = [{"checked": False, **res} for res in results]
                                        except Exception as e:
                                            st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                                    else:
                                        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                                    st.rerun()
                            with fw_c2:
                                add_button_disabled = st.session_state[f"fw_search_results_{i}"] is None
                                if st.button("é¸æŠè¡Œã‚’è¿½åŠ ", key=f"fw_add_results_{i}", disabled=add_button_disabled):
                                    selected_fw_results = [res for res in st.session_state[f"fw_search_results_{i}"] if res["checked"]]
                                    if not st.session_state.segmented_sentences[i].get("search_results"):
                                        st.session_state.segmented_sentences[i]["search_results"] = []
                                    # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦è¿½åŠ 
                                    for res in selected_fw_results: res["checked"] = False
                                    st.session_state.segmented_sentences[i]["search_results"] = selected_fw_results + st.session_state.segmented_sentences[i]["search_results"]
                                    st.session_state[f"show_fw_search_{i}"] = False # UIã‚’é–‰ã˜ã‚‹
                                    st.rerun()
                            with fw_c3:
                                if st.button("é–‰ã˜ã‚‹", key=f"fw_close_{i}"):
                                    st.session_state[f"show_fw_search_{i}"] = False
                                    st.rerun()

                            # ### å¤‰æ›´ ### ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã‚’è©³ç´°è¡¨ç¤º
                            if st.session_state[f"fw_search_results_{i}"] is not None:
                                st.markdown("---")
                                st.markdown("##### æ¤œç´¢çµæœ")
                                is_ja_q_for_highlight = is_japanese(fw_query)

                                for fw_idx, fw_res in enumerate(st.session_state[f"fw_search_results_{i}"]):
                                    with st.container(border=True):
                                        check_col, content_col = st.columns([0.08, 0.92])
                                        with check_col:
                                            is_checked = st.checkbox(" ", value=fw_res.get("checked", False), key=f"fw_res_check_{i}_{fw_idx}", label_visibility="collapsed")
                                            st.session_state[f"fw_search_results_{i}"][fw_idx]["checked"] = is_checked

                                        with content_col:
                                            res_en, res_ja, source_file = fw_res.get("en_text", ""), fw_res.get("jp_text", ""), fw_res.get("sourceFile", "")
                                            highlights = fw_res.get("@search.highlights") or {}
                                            en_snips, ja_snips = highlights.get("en_text", []), highlights.get("jp_text", [])
                                            jp_title = fw_res.get("jp_title", "")
                                            source_file_display = source_file.replace(".csv", ".pdf")
                                            title_prefix = f"**{jp_title}**" if jp_title else ""
                                            valid_date_str = fw_res.get("valid_date", "")
                                            date_display = ""
                                            if valid_date_str:
                                                try:
                                                    formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                                                    date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
                                                except (ValueError, TypeError):
                                                    date_display = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"
                                            metadata_str = f"{title_prefix}{date_display} | Source: **{source_file_display}#{fw_res['line_number']}** | Score: {fw_res['@search.score']:.4f}"
                                            res_c1, res_c2 = st.columns([0.8, 0.2])
                                            with res_c1: st.markdown(metadata_str)
                                            with res_c2: st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’é–‹ã</a>', unsafe_allow_html=True)

                                            if is_ja_q_for_highlight:
                                                ja_html_highlighted = merge_server_highlights(res_ja, ja_snips) if ja_snips else client_side_highlight(res_ja, fw_query)
                                                en_html_highlighted = _escape_html(res_en)
                                            else:
                                                en_html_highlighted = merge_server_highlights(res_en, en_snips) if en_snips else client_side_highlight(res_en, fw_query)
                                                ja_html_highlighted = _escape_html(res_ja)
                                            st.markdown(f"**è‹±èªåŸæ–‡:**<br>{en_html_highlighted}", unsafe_allow_html=True)
                                            st.markdown(f"**æ—¥æœ¬èªè¨³:**<br>{ja_html_highlighted}", unsafe_allow_html=True)


                    # 1. AIç¿»è¨³çµæœ
                    if "ai_translation" in sentence_data and sentence_data["ai_translation"]:
                        st.markdown("---")
                        st.markdown("ğŸ”¤**AIç¿»è¨³çµæœ:**")
                        translation_data = sentence_data["ai_translation"]
                        with st.container(border=True):
                            display_text = f"{translation_data['text']} (ç¿»è¨³ã‚¹ã‚³ã‚¢: {translation_data['score']:.2f})"
                            st.markdown(display_text.replace('\n', '  \n'))
                        translated_text = translation_data['text']
                        if st.button("ğŸ“ å¹³ä»„ç¢ºèªå‡¦ç†", key=f"check_text_{i}"):
                            original_text_to_pass = sentence_data.get('text', '')
                            url_to_open = f"?check_text={urllib.parse.quote(translated_text)}&original_text={urllib.parse.quote(original_text_to_pass)}"

                            st.components.v1.html(f"<script>window.open('{url_to_open}', '_blank');</script>", height=0)

                    # 2. é©ç”¨ã™ã‚‹è¾æ›¸ç”¨èª
                    if "found_terms" in sentence_data:
                        st.markdown("---")
                        if sentence_data["found_terms"]:
                            st.markdown("ğŸ“–**é©ç”¨ã™ã‚‹è¾æ›¸ç”¨èªã‚’é¸æŠ:**")
                            with st.container(border=True):
                                header_cols = st.columns([0.1, 0.45, 0.45])
                                header_cols[0].markdown("**é©ç”¨**")
                                header_cols[1].markdown("**è‹±èªåŸæ–‡**")
                                header_cols[2].markdown("**æ—¥æœ¬èªè¨³**")
                                for term_idx, term_data in enumerate(sentence_data["found_terms"]):
                                    st.markdown("<div style='background-color: #ddd; height: 1px; margin: 10px 0;'></div>", unsafe_allow_html=True)
                                    row_cols = st.columns([0.1, 0.45, 0.45], vertical_alignment="center")
                                    with row_cols[0]:
                                        is_checked = st.checkbox(" ", value=term_data["checked"], key=f"term_check_{i}_{term_idx}", label_visibility="collapsed")
                                        st.session_state.segmented_sentences[i]["found_terms"][term_idx]["checked"] = is_checked
                                    with row_cols[1]:
                                        en_term = term_data["en"]
                                        en_url = f"?search_term={urllib.parse.quote(en_term)}"
                                        st.markdown(f'<a href="{en_url}" target="_blank" style="text-decoration: none;">{en_term}</a>', unsafe_allow_html=True)
                                    with row_cols[2]:
                                        ja_term = term_data["ja"]
                                        ja_url = f"?search_term={urllib.parse.quote(ja_term)}"
                                        st.markdown(f'<a href="{ja_url}" target="_blank" style="text-decoration: none;">{ja_term}</a>', unsafe_allow_html=True)
                        else:
                            st.info("ğŸ“– è©²å½“ã™ã‚‹ç™»éŒ²è¾æ›¸ç”¨èªã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                    # 3. é¡ä¼¼æ–‡æ¤œç´¢çµæœ
                    if sentence_data.get("search_results"):
                        st.markdown("---")
                        st.markdown("ğŸ”**é¡ä¼¼æ–‡æ¤œç´¢çµæœ:**ï¼ˆç¿»è¨³ã®å‚ç…§ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹è¡Œã‚’é¸æŠã—ã¦ãã ã•ã„ï¼‰")
                        query_for_highlight = sentence_data['text']
                        is_ja_q_for_highlight = st.session_state.get("is_last_query_ja", is_japanese(query_for_highlight))
                        for j, result_item in enumerate(sentence_data["search_results"]):
                            with st.container(border=True):
                                check_col, content_col = st.columns([0.08, 0.92])
                                with check_col:
                                    is_checked = st.checkbox(" ", value=result_item.get("checked", False), key=f"res_check_{i}_{j}", label_visibility="collapsed", help="ã“ã®è¡Œã‚’ç¿»è¨³ã®å‚ç…§ã«å«ã‚ã‚‹")
                                    st.session_state.segmented_sentences[i]["search_results"][j]["checked"] = is_checked
                                with content_col:
                                    res_en, res_ja, source_file = result_item.get("en_text", ""), result_item.get("jp_text", ""), result_item.get("sourceFile", "")
                                    highlights = result_item.get("@search.highlights") or {}
                                    en_snips, ja_snips = highlights.get("en_text", []), highlights.get("jp_text", [])
                                    jp_title_tab2 = result_item.get("jp_title", "")
                                    source_file_display_tab2 = source_file.replace(".csv", ".pdf")
                                    title_prefix_tab2 = f"**{jp_title_tab2}**" if jp_title_tab2 else ""
                                    valid_date_str = result_item.get("valid_date", "")
                                    date_display_tab2 = ""
                                    if valid_date_str:
                                        try:
                                            formatted_date = datetime.fromisoformat(valid_date_str.replace('Z', '+00:00')).strftime('%Yå¹´%mæœˆ%dæ—¥')
                                            date_display_tab2 = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{formatted_date}**"
                                        except (ValueError, TypeError):
                                            date_display_tab2 = f" | åŠ¹åŠ›ç™ºç”Ÿæ—¥: **{valid_date_str}**"
                                    metadata_str = f"{title_prefix_tab2}{date_display_tab2} | Source: **{source_file_display_tab2}#{result_item['line_number']}** | Score: {result_item['@search.score']:.4f}"
                                    res_col1_tab2, res_col2_tab2 = st.columns([0.8, 0.2])
                                    with res_col1_tab2: st.markdown(metadata_str)
                                    with res_col2_tab2: st.markdown(f'<a href="?view_treaty={urllib.parse.quote(source_file)}" target="_blank" rel="noopener noreferrer">æ¡ç´„å…¨æ–‡ã‚’é–‹ã</a>', unsafe_allow_html=True)
                                    if is_ja_q_for_highlight:
                                        ja_html_highlighted = merge_server_highlights(res_ja, ja_snips) if ja_snips else client_side_highlight(res_ja, query_for_highlight)
                                        en_html_highlighted = _escape_html(res_en)
                                    else:
                                        en_html_highlighted = merge_server_highlights(res_en, en_snips) if en_snips else client_side_highlight(res_en, query_for_highlight)
                                        ja_html_highlighted = _escape_html(res_ja)
                                    st.markdown(f"**è‹±èªåŸæ–‡:**<br>{en_html_highlighted}", unsafe_allow_html=True)
                                    st.markdown(f"**æ—¥æœ¬èªè¨³:**<br>{ja_html_highlighted}", unsafe_allow_html=True)

    with tab_maintenance:
        display_maintenance_page()